BERT : Bi-Directional Encoder Re-Presentation from transformers 

1:) Learning  objectives
- Understand the architecture and components of BERT.
- Learn the preprocessing steps required for BERT input and how to handle varying input sequence lengths.
- Gain practical knowledge of implementing BERT using popular machine learning frameworks like TensorFlow or PyTorch.
Learn how to fine-tune BERT for specific downstream tasks, such as text classification or named entity recognition.

BERT Archietecture : 

1) BERT-Base (Cased / Un-Cased): 12-layer, 768-hidden-nodes, 12-attention-heads, 110M parameters

2) BERT-Large (Cased / Un-Cased): 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters

![alt text](https://github.com/[datamind321]/[bert-model-nlp]/asset/0_ViwaI3Vvbnd-CJSQ.webp?raw=true)

We are implement BERT-Base and implement of that's model.

Happy Learning :-)
